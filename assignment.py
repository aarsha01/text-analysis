# -*- coding: utf-8 -*-
"""assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/179OnAwqT58uq0YqWuFwNJp-4Iv-27sHf
"""
#steps to run
#change the directory to root folder
#to run -python assignment.py
#include all the dependeicies required
#import necessary files
import pandas as pd
from bs4 import BeautifulSoup
import requests
import numpy as np
import os
import textwrap
import glob
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')
import re

# Reading the Excel file
df = pd.read_excel('Input.xlsx', usecols='A, B')
df = df.iloc[:101]  # Selecting the first 101 rows for processing


headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36"
}

# Create a folder to store extracted data, if it doesn't exist
foldername = 'extracted_datas'
os.makedirs(foldername, exist_ok=True)

# Iterate through rows in the dataframe
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Send an HTTP request to the URL to get the webpage content
    page = requests.get(url, headers=headers)  # loading text 
    soup = BeautifulSoup(page.content, 'html.parser')  # parsing URL text


    if page.status_code == 200:

        # Extract the title of the website
        title = soup.find('h1', attrs={'class': 'tdb-title-text'})  # extracting title of website

        # If the first attempt to find the title fails, try an alternative class
        if not title:
            title = soup.find('h1', attrs={'class': 'entry-title'})  # extracting title of website

        title = title.string

        # Check if content is present before trying to access it
        contentParent = soup.find('div', attrs={'class': 'tdb_single_content'})

        if contentParent:
            content = contentParent.findChild('div', attrs={'class':'tdb-block-inner'}).find_all(string=True, recursive=True)
        else:
            content = soup.find('div', attrs={'class':'td-post-content tagdiv-type'}).find_all(string=True, recursive=True)

        # If content is found, create a text file and write the content to it
        if content:
            text = title + '\n'
            for p_tag in content:
                text += textwrap.fill(p_tag.string, width=100)

            # Save the content to a text file
            filename = os.path.join(foldername, f"{url_id}.txt")
            with open(filename, 'w', encoding='utf-8') as file:
                file.write(text)
            print(url_id)

        else:
            print(f"No content found for URL ID {url_id}")
    else:
        print(f"Failed to fetch content for URL ID {url_id}")

# Directory containing stop word files
stop_word_dir = 'StopWords/'

# Set to store stop words
stop_words = set()

# Get a list of all stop word files in the specified directory
stopwords_dir = glob.glob(stop_word_dir + '*.txt')

# Iterate through each stop word file
for file in stopwords_dir:
  
    with open(file, 'r', encoding='ISO-8859-1') as f:
        text = f.read().splitlines()
        stop_words.update(set(word.lower() for word in text))

# Print the set of stop words
print(stop_words)


textfiles = []
text_file_dir = 'extracted_datas/'
textfiles_dir = glob.glob(text_file_dir + '*')

# Iterate through each text file
for txt in textfiles_dir:
    # Open each text file and read its content
    with open(txt, 'r', encoding='utf8') as f:
        print(f)
        text = f.read()
        tokens = word_tokenize(text)
        filtered_text = [word for word in tokens if word.lower() not in stop_words]
        textfiles.append(filtered_text)

# Print the tokens from the last text file (for demonstration purposes)
print(tokens)

# Print the filtered text from all text files
print(textfiles)

# Directory containing positive and negative master dictionaries
master_dic_dir = 'MasterDictionary/'

# Sets to store positive and negative words
pos = set()
neg = set()

# Get a list of all master dictionary files in the specified directory
masterdicts_dir = glob.glob(master_dic_dir + '*.txt')


for file in masterdicts_dir:
   
    with open(file, 'r', encoding='ISO-8859-1') as f:
        words = f.read().splitlines()

    # Update the positive set if the file corresponds to positive words
    if "positive-words.txt" in file:
        pos.update(words)
    # Update the negative set if the file corresponds to negative words
    elif "negative-words.txt" in file:
        neg.update(words)


print("Positive Set:", pos)
print("Negative Set:", neg)

# Lists to store positive and negative words, positive score, negative score, polarity score, and subjectivity score
positive_words = []
negative_words = []
positive_score = []
negative_score = []
polarity_score = []
subjectivity_score = []

# Iterate through each tokenized and filtered text
for i in range(len(textfiles)):
    # Extract positive words from the current text file
    positive_words.append([word for word in textfiles[i] if word.lower() in pos])

    # Extract negative words from the current text file
    negative_words.append([word for word in textfiles[i] if word.lower() in neg])

    # Calculate positive score for the current text file
    positive_score.append(len(positive_words[i]))

    # Calculate negative score for the current text file
    negative_score.append(len(negative_words[i]))

    # Calculate polarity score for the current text file
    polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))

    # Calculate subjectivity score for the current text file
    subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(textfiles[i])) + 0.000001))

# Print the calculated polarity scores
print(polarity_score)

from nltk.corpus import stopwords
import os
import re

# Define stopwords as a list
stopwords_list = stopwords.words('english')

# Lists to store various readability metrics
avg_sentence_length = []
Percentage_of_Complex_words = []
Fog_Index = []
complex_word_count = []
avg_word_per_sent = []

def measure(file):
    # Read the content of the text file
    with open(os.path.join(text_file_dir, file), 'r', encoding='utf8') as f:
        text = f.read()

    # Remove punctuations
    text = re.sub(r'[^\w\s.]', '', text)

    # Split the given text file into sentences
    sentences = text.split('.')

    # Total number of sentences in a file
    num_sentences = len(sentences)

    # Total words in the file (excluding stop words)
    words = [word for word in text.split() if word.lower() not in stop_words]
    num_words = len(words)

    # Calculate average sentence length
    avg_sentence_len = num_words / num_sentences
    avg_word_per_sent = avg_sentence_len  # Using the same variable for clarity

    # Identify complex words (syllable count greater than 2)
    complex_words = [word for word in words if sum(1 for letter in word if letter.lower() in 'aeiou') > 2]

    # Calculate percentage of complex words
    Percent_Complex_words = len(complex_words) / num_words

    # Calculate Fog Index
    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)

    # Return the calculated readability metrics
    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words), avg_word_per_sent

# iterate through each text files
for file in os.listdir(text_file_dir):
    x, y, z, a, b = measure(file)
    avg_sentence_length.append(x)
    Percentage_of_Complex_words.append(y)
    Fog_Index.append(z)
    complex_word_count.append(a)
    avg_word_per_sent.append(b)

# Print or use the collected results as needed
print("Average Sentence Length:", avg_sentence_length)
print("Percentage of Complex Words:", Percentage_of_Complex_words)
print("Fog Index:", Fog_Index)
print("Complex Word Count:", complex_word_count)
print("Average Number of Words Per Sentence:", avg_word_per_sent)

def cleaned_words(file):
  
    with open(os.path.join(text_file_dir, file), 'r', encoding='utf8') as f:
        text = f.read()

    # Remove punctuations
    text = re.sub(r'[^\w\s]', '', text)

    # Remove stop words and split into words
    words = [word for word in text.split() if word.lower() not in stopwords_list]

    return len(words)

# List to store word count for each text file
word_count = []

for file in os.listdir(text_file_dir):
    x = cleaned_words(file)
    word_count.append(x)

print("Word Count:", word_count)

# Set of English stopwords
stopwords_set = set(stopwords.words('english'))

# List to store average syllable word count for each text file
avg_syllable_word_list = []

for files in os.listdir(text_file_dir):
    with open(os.path.join(text_file_dir, files), 'r', encoding='utf8') as f:
        text = re.sub(r'[^\w\s.]', '', f.read())

        # Remove stopwords and split into words
        words = [word for word in text.split() if word.lower() not in stopwords_set]

        # Initialize variables for syllable count and syllable words
        syllable_count = 0
        syllable_words = []

        # Calculate syllable count for each word in the text file
        for word in words:
            if word.endswith('es'):
                word = word[:-2]
            elif word.endswith('ed'):
                word = word[:-2]

            # Count vowels in the word to estimate syllable count
            vowels = 'aeiou'
            syllable_count_word = sum(1 for letter in word if letter.lower() in vowels)

            if syllable_count_word >= 1:
                syllable_words.append(word)
                syllable_count += syllable_count_word

        # Calculate average syllable word count for the text file
        avg_syllable_word_count = syllable_count / len(syllable_words)
        avg_syllable_word_list.append(avg_syllable_word_count)


print("Average Syllable Word Count:", avg_syllable_word_list)

#personal pronouns calculation
def count_personal_pronouns(file):
  with open(os.path.join(text_file_dir,file), 'r', encoding='utf8') as f:
    text = f.read()
    personal_pronouns = ["I", "we", "my", "ours", "us"]
    count = 0
    for pronoun in personal_pronouns:
      count += len(re.findall(r"\b" + pronoun + r"\b", text)) # \b is used to match word boundaries
  return count

pp_count = []
for file in os.listdir(text_file_dir):
  x = count_personal_pronouns(file)
  pp_count.append(x)

# List to store average word length for each text file
avg_word_length_list = []


for files in os.listdir(text_file_dir):
    with open(os.path.join(text_file_dir, files), 'r', encoding='utf8') as f:
        text = re.sub(r'[^\w\s.]', '', f.read())

        # Remove stopwords and split into words
        words = [word for word in text.split() if word.lower() not in stopwords_set]

        # Calculate total character count and total word count for the text file
        total_char_count = sum(len(word) for word in words)
        total_word_count = len(words)

        # Calculate average word length for the text file
        if total_word_count > 0:
            avg_word_length = total_char_count / total_word_count
            avg_word_length_list.append(avg_word_length)


print("Average Word Length List:", avg_word_length_list)

#Reading output excel file
output_df = pd.read_excel('Output Data Structure.xlsx')
#displaying first 40 data
output_df.head(40)

# Drop rows with indices 35 and 48 to drop url id with values blackassign0036 and blackassign0049 as the page dont exist
output_df.drop([35, 48], inplace=True)

# These are the required parameters
variables = [positive_score,
              negative_score,
              polarity_score,
              subjectivity_score,
              avg_sentence_length,
              Percentage_of_Complex_words,
              Fog_Index,
              avg_word_per_sent,
              complex_word_count,
              word_count,
              avg_syllable_word_list,
              pp_count,
              avg_word_length_list]

# Write the values to the dataframe
for i, var in enumerate(variables):
    output_df.iloc[:, i + 2] = var

# Save the dataframe to the disk
output_df.to_csv('Output_Data.csv', index=False)



